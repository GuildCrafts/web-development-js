---
authors:
  - "@lumodon"
teamSize: 2
issueNumber: 102
---

# Build a Node.js Web Crawler / Web Scraper

## Description

"Web scraping is a technique in data extraction where you pull information from websites." *1
Create a web scraper which gathers information from the web.  The tutorial listed below will take you through, step by step, in setting up your own crawler. You should modify this tutorial to fit some subject of interest to you. 

Get with your team and brainstorm what types of information or data would be a good fit for scraping? Note that information which is presented such as images in a list of pokemon is best suited, and not, for example, stocks from the stock market, or other types of data which is fetched from a singular database (In which case, you would be better off hoping the host has an API such as Google Books API)
## Context

There are many common / practical uses to this, and this is a technique employed by many companies these days. What types of data are a good fit for scraping? Why some types and not others? This project also gives you a great practical example of the limits that efficient code can bring, since you will be pushing the limit of a process when your crawler is running.
## Specifications

Using https://scotch.io/tutorials/scraping-the-web-with-node-js as a resource, optionally with the same libraries, preferably explore alternative libraries that will still allow you to accomplish the same end result.
- [ ] Complete all the steps in the tutorial.
- [ ] Spec two.
- [ ] Spec three.
### Required
- [ ] The artifact produced is properly licensed, preferably with the [MIT license](https://opensource.org/licenses/MIT).
## Quality Rubric
- Quality rubric one: point value
- Quality rubric two: point value
- Quality rubric three: point value

---






*1: Kukic, Ado. "Scraping the Web With Node.js." Scotch. The Scotch Family, 13 Mar. 2014. Web. https://scotch.io/tutorials/scraping-the-web-with-node-js. Accessed 26 Oct. 2016. 
# TODO
## Will list review and edit ideas to perform on this project over the weekend here:
- [ ] Talk about distributed scraping, legal gray areas, server overloading, and potential for being (mistakenly) identified as a DDoS when doing innocous scraping, and using sleep functions to minimize 'damage' (better word for damage?)
